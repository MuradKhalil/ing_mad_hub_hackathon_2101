{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ING Hubs Hackaton\n",
    "\n",
    "29.01.2021\n",
    "\n",
    "Jaime Elguero, Santiago Gil, Borja Serra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AIGym\n",
    "\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.\n",
    "\n",
    "The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "#env.reset()\n",
    "#env.seed(1); np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mountain Car** is a classic reinforcement learning problem where the objective is to create an algorithm that learns to climb a steep hill to reach the goal marked by a flag. The car’s engine is not powerful enough to drive up the hill without a head start so the car must drive up the left hill to obtain enough momentum to scale the steeper hill to the right and reach the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-89b21acfc490>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Adjust Q value for current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             delta = learning*(reward + \n\u001b[0m\u001b[0;32m     80\u001b[0m                               discount*np.max(Q[state2_adj[0], \n\u001b[0;32m     81\u001b[0m                                                 state2_adj[1]]) - \n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning' is not defined"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 0.2\n",
    "discount = 0.9\n",
    "epsilon  = 0.8\n",
    "min_eps  = 0\n",
    "episodes = 10000\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "# Determine size of discretized state space\n",
    "num_states = (env.observation_space.high - env.observation_space.low) * np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "\n",
    "# Initialize Q table\n",
    "Q = np.random.uniform(low = 0, high = 0, \n",
    "                      size = (num_states[0], num_states[1], \n",
    "                              env.action_space.n))\n",
    "\n",
    "# Initialize Q table\n",
    "revisited = np.random.uniform(low = 0, high = 0,\n",
    "                              size = (num_states[0], num_states[1]))\n",
    "\n",
    "# Initialize variables to track rewards\n",
    "reward_list = []\n",
    "ave_reward_list = []\n",
    "\n",
    "max_position = state[0]\n",
    "positions = np.ndarray([0,2])\n",
    "successful = []\n",
    "position = []\n",
    "\n",
    "# Calculate episodic reduction in epsilon\n",
    "reduction = (epsilon - min_eps)/episodes\n",
    "\n",
    "# Run Q learning algorithm\n",
    "for i in range(episodes):\n",
    "        \n",
    "    # Initialize parameters\n",
    "    done = False\n",
    "    tot_reward, reward = 0,0\n",
    "    state = env.reset()\n",
    "    \n",
    "    max_position_episode = state[0]\n",
    "\n",
    "    # Discretize state\n",
    "    state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "    state_adj = np.round(state_adj, 0).astype(int)\n",
    "\n",
    "    while done != True:\n",
    "        # Render environment for last five episodes\n",
    "        #if i >= (episodes - 5):\n",
    "        #    env.render()\n",
    "\n",
    "        # Determine next action - epsilon greedy strategy\n",
    "        if np.random.random() < 1 - epsilon:\n",
    "            action = np.argmax(Q[state_adj[0], state_adj[1]])\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        # Get next state and reward\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        \n",
    "        if state2[0] > max_position:\n",
    "            max_position = state2[0]\n",
    "            positions = np.append(positions, [[i, max_position]], axis=0)\n",
    "    \n",
    "        # Discretize state2\n",
    "        state2_adj = (state2 - env.observation_space.low) * np.array([10, 100])\n",
    "        state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "\n",
    "        # Allow for terminal states\n",
    "        if done:\n",
    "            position.append(state2[0])\n",
    "            if state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                successful.append(i)\n",
    "\n",
    "        # Adjust Q value for current state\n",
    "        else:\n",
    "            delta = learning*(reward + \n",
    "                              discount*np.max(Q[state2_adj[0], \n",
    "                                                state2_adj[1]]) - \n",
    "                              Q[state_adj[0], state_adj[1], action])\n",
    "            Q[state_adj[0], state_adj[1],action] += delta\n",
    "            revisited[state_adj[0], state_adj[1]] += 1\n",
    "\n",
    "        # Update variables\n",
    "        tot_reward += reward\n",
    "        state_adj = state2_adj\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > min_eps:\n",
    "        epsilon -= reduction\n",
    "\n",
    "    # Track rewards\n",
    "    reward_list.append(tot_reward)\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        ave_reward = np.mean(reward_list)\n",
    "        ave_reward_list.append(ave_reward)\n",
    "        reward_list = []\n",
    "        print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Run Q-learning algorithm\n",
    "#rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=[10,5])\n",
    "p = pd.Series(position)\n",
    "ma = p.rolling(10).mean()\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(ma)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Car Final Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Rewards\n",
    "plt.subplots(figsize=(10,5))\n",
    "plt.plot(100*(np.arange(len(ave_reward_list)) + 1), ave_reward_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Furthest Position: {}'.format(max_position))\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(positions[:,0], positions[:,1])\n",
    "ax2.hist(successful, bins = int(episodes/500));\n",
    "ax.set_xlabel('Episodes');\n",
    "ax.set_ylabel('Furthest Position');\n",
    "ax2.set_ylabel('#Successful');\n",
    "print('successful episodes: {}'.format(np.count_nonzero(successful)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.rand(10, 10) * 20\n",
    "\n",
    "# create discrete colormap\n",
    "cmap = colors.ListedColormap(['red', 'blue', 'green', 'black'])\n",
    "bounds = [0,0.5,1.5,2.5,3]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "Q_colors = Q.argmax(axis=2)\n",
    "Q_colors[(revisited/revisited.sum()) < 0.0001] = 4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(Q_colors.shape))\n",
    "ax.imshow(Q_colors, cmap=cmap, norm=norm)\n",
    "\n",
    "# draw gridlines\n",
    "ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
    "ax.set_xticks(np.arange(-.5, Q_colors.shape[1], 1));\n",
    "ax.set_xticklabels(np.round(np.arange(env.observation_space.low[1], env.observation_space.high[1], 0.01), 2))\n",
    "ax.set_xlabel('velocity')\n",
    "ax.set_yticks(np.arange(-.5, Q_colors.shape[0], 1));\n",
    "ax.set_yticklabels(np.round(np.arange(env.observation_space.low[0], env.observation_space.high[0], 0.1), 2))\n",
    "ax.set_ylabel('position')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#        0 - Red     Accelerate to the Left\n",
    "#        1 - Blue    Don't accelerate\n",
    "#        2 - Green   Accelerate to the Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweaking it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 1.0  # initial learning rate\n",
    "min_learning_rate = 0.001    # minimum learning rate\n",
    "discount = 1\n",
    "epsilon  = 0.1\n",
    "min_eps  = 0\n",
    "episodes = 10000\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "# Determine size of discretized state space\n",
    "num_states = (env.observation_space.high - env.observation_space.low) * np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "\n",
    "# Initialize Q table\n",
    "Q = np.random.uniform(low = 0, high = 0, \n",
    "                      size = (num_states[0], num_states[1], \n",
    "                              env.action_space.n))\n",
    "\n",
    "# Initialize Q table\n",
    "revisited = np.random.uniform(low = 0, high = 0,\n",
    "                              size = (num_states[0], num_states[1]))\n",
    "\n",
    "# Initialize variables to track rewards\n",
    "reward_list = []\n",
    "ave_reward_list = []\n",
    "\n",
    "max_position = state[0]\n",
    "positions = np.ndarray([0,2])\n",
    "successful = []\n",
    "position = []\n",
    "\n",
    "# Calculate episodic reduction in epsilon\n",
    "reduction = (epsilon - min_eps)/episodes\n",
    "\n",
    "# Run Q learning algorithm\n",
    "for i in range(episodes):\n",
    "    \n",
    "    learning = max(min_learning_rate, initial_learning_rate * (0.85 ** (i//100)))\n",
    "    \n",
    "    # Initialize parameters\n",
    "    done = False\n",
    "    tot_reward, reward = 0,0\n",
    "    state = env.reset()\n",
    "    \n",
    "    max_position_episode = state[0]\n",
    "\n",
    "    # Discretize state\n",
    "    state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "    state_adj = np.round(state_adj, 0).astype(int)\n",
    "\n",
    "    while done != True:\n",
    "        # Render environment for last five episodes\n",
    "        #if i >= (episodes - 5):\n",
    "        #    env.render()\n",
    "\n",
    "        # Determine next action - epsilon greedy strategy\n",
    "        if np.random.random() < 1 - epsilon:\n",
    "            action = np.argmax(Q[state_adj[0], state_adj[1]])\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        # Get next state and reward\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        \n",
    "        if state2[0] > max_position:\n",
    "            max_position = state2[0]\n",
    "            positions = np.append(positions, [[i, max_position]], axis=0)\n",
    "\n",
    "        #if state2[0] > max_position_episode:\n",
    "        #    max_position_episode = state2[0]\n",
    "        #    reward = state2[0] + 0.5\n",
    "    \n",
    "        # Discretize state2\n",
    "        state2_adj = (state2 - env.observation_space.low) * np.array([10, 100])\n",
    "        state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "\n",
    "        # Allow for terminal states\n",
    "        if done:\n",
    "            position.append(state2[0])\n",
    "            if state2[0] >= 0.5:\n",
    "                #reward += 1\n",
    "                epsilon *= .99\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                successful.append(i)\n",
    "\n",
    "        # Adjust Q value for current state\n",
    "        else:\n",
    "            delta = learning*(reward + \n",
    "                              discount*np.max(Q[state2_adj[0], \n",
    "                                                state2_adj[1]]) - \n",
    "                              Q[state_adj[0], state_adj[1], action])\n",
    "            Q[state_adj[0], state_adj[1],action] += delta\n",
    "            revisited[state_adj[0], state_adj[1]] += 1\n",
    "\n",
    "        # Update variables\n",
    "        tot_reward += reward\n",
    "        state_adj = state2_adj\n",
    "\n",
    "    # Decay epsilon\n",
    "    #if epsilon > min_eps:\n",
    "    #    epsilon -= reduction\n",
    "\n",
    "    # Track rewards\n",
    "    reward_list.append(tot_reward)\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        ave_reward = np.mean(reward_list)\n",
    "        ave_reward_list.append(ave_reward)\n",
    "        reward_list = []\n",
    "        print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Run Q-learning algorithm\n",
    "#rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=[10,5])\n",
    "p = pd.Series(position)\n",
    "ma = p.rolling(10).mean()\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(ma)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Car Final Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Rewards\n",
    "plt.subplots(figsize=(10,5))\n",
    "plt.plot(100*(np.arange(len(ave_reward_list)) + 1), ave_reward_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Furthest Position: {}'.format(max_position))\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(positions[:,0], positions[:,1])\n",
    "ax2.hist(successful, bins = int(episodes/500));\n",
    "ax.set_xlabel('Episodes');\n",
    "ax.set_ylabel('Furthest Position');\n",
    "ax2.set_ylabel('#Successful');\n",
    "print('successful episodes: {}'.format(np.count_nonzero(successful)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_colors = Q.argmax(axis=2)\n",
    "Q_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.rand(10, 10) * 20\n",
    "\n",
    "# create discrete colormap\n",
    "cmap = colors.ListedColormap(['red', 'blue', 'green', 'black'])\n",
    "bounds = [0,0.5,1.5,2.5,3]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "Q_colors = Q.argmax(axis=2)\n",
    "Q_colors[(revisited/revisited.sum()) < 0.001] = 4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(Q_colors.shape))\n",
    "ax.imshow(Q_colors, cmap=cmap, norm=norm)\n",
    "\n",
    "# draw gridlines\n",
    "ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
    "ax.set_xticks(np.arange(-.5, Q_colors.shape[1], 1));\n",
    "ax.set_xticklabels(np.round(np.arange(env.observation_space.low[1], env.observation_space.high[1], 0.01), 2))\n",
    "ax.set_xlabel('velocity')\n",
    "ax.set_yticks(np.arange(-.5, Q_colors.shape[0], 1));\n",
    "ax.set_yticklabels(np.round(np.arange(env.observation_space.low[0], env.observation_space.high[0], 0.1), 2))\n",
    "ax.set_ylabel('position')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#        0 - Red     Accelerate to the Left\n",
    "#        1 - Blue    Don't accelerate\n",
    "#        2 - Green   Accelerate to the Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Score posted!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hackathon_mad_hub_2101 as hack_tools\n",
    "\n",
    "team = 'La Macarena'\n",
    "score = 69\n",
    "\n",
    "hack_tools.post_score(team, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
